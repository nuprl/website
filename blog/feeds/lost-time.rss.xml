<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>PRL Blog: Posts tagged 'lost time'</title>
  <description>PRL Blog: Posts tagged 'lost time'</description>
  <link>http://prl.ccs.neu.edu/blog/tags/lost-time.html</link>
  <lastBuildDate>Wed, 03 Aug 2016 14:09:02 UT</lastBuildDate>
  <pubDate>Wed, 03 Aug 2016 14:09:02 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>A few cores too many</title>
   <link>http://prl.ccs.neu.edu/blog/2016/08/03/a-few-cores-too-many/?utm_source=lost-time&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2016-08-03-a-few-cores-too-many</guid>
   <pubDate>Wed, 03 Aug 2016 14:09:02 UT</pubDate>
   <author>PRL</author>
   <description>
&lt;p&gt;Performance matters for software systems, but performance is not always easy  to measure. At the PRL we recently had a scare with some unreliable measurements. Here is the story.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Last year, we proposed a method for evaluating the performance of gradual type  systems based on measuring &lt;em&gt;every possible configuration&lt;/em&gt; of typed and untyped  code that a programmer might explore &lt;a href="http://www.ccs.neu.edu/racket/pubs/popl16-tfgnvf.pdf"&gt;(pdf)&lt;/a&gt;. Given the freedom that gradual typing offers, this is the only realistic way to measure  the performance of a gradual type system.&lt;/p&gt;

&lt;p&gt;But it is a lot to measure! While developing the method, we spent over 3 months benchmarking a total of 75,844 configurations. Each configuration is a complete program and some gradual typings caused  some programs to slow by 50x or even 100x, so many of these configurations took  minutes to run.&lt;/p&gt;

&lt;p&gt;The next question we asked was naturally &amp;ldquo;how can we scale this method to large software projects?&amp;rdquo; In &lt;a href="http://docs.racket-lang.org/ts-reference/Libraries_Provided_With_Typed_Racket.html#%28part._.Porting_.Untyped_.Modules_to_.Typed_.Racket%29"&gt;our case&lt;/a&gt;,  the number of gradually typed configurations scaled exponentially with the number of modules. Current gradual type system for &lt;a href="https://github.com/mvitousek/reticulated"&gt;Python&lt;/a&gt;  and &lt;a href="http://www.di.ens.fr/~zappa/readings/ecoop15.pdf"&gt;JavaScript&lt;/a&gt;  are exponential in the number of &lt;em&gt;variables&lt;/em&gt; in the program.&lt;/p&gt;

&lt;p&gt;We explored two solutions:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Max New began work on a prediction model (inspired by work  on &lt;a href="http://subs.emis.de/LNI/Proceedings/Proceedings213/185.pdf"&gt;software product lines&lt;/a&gt;)  to estimate the performance of &lt;code&gt;2^N&lt;/code&gt; configurations after polynomially-many measurements.&lt;/li&gt;
 &lt;li&gt;Asumu Takikawa and I shopped for a multi-core computer.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;By Thanksgiving, we had bought a Linux machine with 2 &lt;a href="http://www.amd.com/en-us/products/server/opteron/6000/6300"&gt;AMD Opteron 6376 2.3GHz&lt;/a&gt;  processors (16 cores each) and put it to work running benchmarks on 29 cores simultaneously. Life was good.&lt;/p&gt;

&lt;p&gt;Later that winter, Max implemented a prediction algorithm. The basic idea was to focus on &lt;em&gt;boundaries&lt;/em&gt; between modules and isolate their  effect on performance. If two modules are untyped, their boundary will have zero cost. If the same two modules are typed, their boundary might result in an overall  performance improvement due to type-driven optimizations. And if one module is typed and the other untyped, their boundary will  suffer some cost of type checking at runtime. In general a program with &lt;code&gt;N&lt;/code&gt; modules has at most &lt;code&gt;N(N - 1) / 2&lt;/code&gt; internal boundaries,  so it is far more time-efficient to measure only the boundaries than to benchmark  &lt;code&gt;2^N&lt;/code&gt; gradually typed configurations.&lt;/p&gt;

&lt;p&gt;Fast-forward to March, we had a prototype prediction algorithm and it was time to test. Again using 29 cores (because, why not), we gathered cost/benefit numbers for  one 4-module benchmark and used them to predict performance for its 16 configurations. The results were not very good.&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/img/a-few-cores-too-many-1.png" alt="Figure 1: True running time vs. predicted running time for 16 configurations" /&gt;
 &lt;p class="caption"&gt;Figure 1: True running time vs. predicted running time for 16 configurations&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Those green circles are the ground truth, the average running time after 5 iterations of each config. The blue triangles are what we predicted. Except for configurations 0 and 8, the triangles are FAR off from the truth. Many are even negative &amp;hellip; obviously the algorithm needs work.&lt;/p&gt;

&lt;p&gt;But then, out of frustration, desperation, or just good luck, Max compared the  predictions to ground truth data gathered on a &lt;em&gt;single&lt;/em&gt; core, leaving the other 31  cores idle.&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/img/a-few-cores-too-many-2.png" alt="Figure 2: Predictions made using measurements from a single core" /&gt;
 &lt;p class="caption"&gt;Figure 2: Predictions made using measurements from a single core&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;First off, the red &amp;ldquo;sequential truth&amp;rdquo; dots are slightly closer to the predicted triangles. Second &amp;mdash; and this is the scary part &amp;mdash; the red dots are very different from  the green dots. &lt;em&gt;Running on 1 core vs. 29 cores should not change the measurements!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From here we tried increasing the running time of the benchmark,  removing I/O and system calls,  checking for hyperthreading (ARM cores don&amp;rsquo;t support it),  and even changing the cores&amp;rsquo; CPU governor. The hope was that results taken from 1 core could match results from &lt;code&gt;N&lt;/code&gt; cores,  for some &lt;code&gt;N &amp;gt; 1&lt;/code&gt;. It turns out &lt;code&gt;N = 2&lt;/code&gt; was stable, but even for &lt;code&gt;N = 3&lt;/code&gt; we found graphs like the following:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/img/a-few-cores-too-many-3.png" alt="Figure 3: exact running times. Same-colored dots in each column should be tightly clustered." /&gt;
 &lt;p class="caption"&gt;Figure 3: exact running times. Same-colored dots in each column should be tightly clustered.&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;This data is for the same 16 configurations as the previous two graphs. Green dots are exact running times measured with 25 cores. Red dots are running times measured with 1 core. The red dots are much closer together, and always unimodal. The green dots are evidence that maybe the 32-core machine has, as Jan Vitek  put it, 30 cores too many.&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;&amp;ldquo;Oh my. You think it&amp;rsquo;ll never happen to you. Well, now I&amp;rsquo;ve learned my lesson.&amp;rdquo;&lt;/p&gt;&lt;!-- bg: If anyone knows this quote I will be AMAZED. If anyone can even Google this quote, I'll buy them 2 beers and a pizza.--&gt;&lt;/blockquote&gt;

&lt;p&gt;And so, we said goodbye to the last 4 months of data and started over running at most two cores. The new results are all stable, but still we keep pinching ourselves.&lt;/p&gt;

&lt;p&gt;P.S. the results from &lt;a href="http://www.ccs.neu.edu/racket/pubs/#popl16-tfgnvf"&gt;POPL 2016&lt;/a&gt; are just fine,  as they were not taken on the new machine running more than 2 cores.  If you have time to confirm, that data is in our  &lt;a href="http://www.ccs.neu.edu/home/asumu/artifacts/popl-2016/"&gt;artifact&lt;/a&gt;  and in the &lt;a href="https://github.com/nuprl/gradual-typing-performance/tree/master/paper/popl-2016/data"&gt;gradual-typing-performance&lt;/a&gt; repo.&lt;/p&gt;</description></item></channel></rss>